{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "What:\n",
    "TextRank uses an extractive approach and is an unsupervised graph-based text summarization technique. \n",
    "\n",
    "How: \n",
    "The basic idea implemented by a graph-based ranking model is that of voting or recommendation.\n",
    "When one vertex links to another one, it is basically casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher the importance of that vertex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk kelancaran TextRank ini, diperlukan untuk :\n",
    "1. men-download library stopwords Bahasa Indonesia: Python Sastrawi.\n",
    "    Cara download library:\n",
    "    1. Open terminal\n",
    "    2. write on command line : \"pip install Sastrawi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "import urllib\n",
    "import os, os.path\n",
    "#import modular regular expression\n",
    "import re\n",
    "import string\n",
    "# import StopWordRemoverFactory class\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open txt file and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing methods used:\n",
    "1. decode to UTF-8\n",
    "2. change all to lower case -> consistency purposes especially during stopwords removal process\n",
    "3. remove all numbers -> avoid redundancy\n",
    "4. remove all punctuation -> avoid redundancy\n",
    "5. stopwords (using Python Sastrawi) -> avoid redundancy\n",
    "6. Stemmer (using Python Sastrawi) -> easy filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kotor debu tdk henti bersin awal masuk kamar kesan kamar kumuh wifi rusak lgsg check out\n",
      "oke cuma air wastafel warna keruh mohon baik\n",
      "kamar semut kamar mandi masalah buang air mampetsehingga air luber luar kamar\n",
      "kamar mandi bau air bau\n",
      "kamar bersih nyaman perlu perhati dan trus kroscheck trutama slot card pintu kamar batrai nya habis apa msih full kasi ada tamu lelah habis jalan jauh tunggu luar karena slotcard pintu kamar di perbaikin ulang tks smoga manfaat\n",
      "tak sesuai espektasi kamar sempit pintu kamar mandi rusak cek in tunggu terlalu lama malam suara berisik dr atap kamar nyaman\n",
      "booking traveloka room sedia kembali uang nya lamaaa\n",
      "pesan kamar double bad dapat kamar twins bad\n",
      "buruk kasur bekas sperma seprai jg air bau karat kasur per jg rusak sabun habis pasword wifi ga tau\n",
      "baik puas masih kurang waktu boking airy rooms pilih bad tp saya cek in kasih bad mohon di tingkat terimah kasih airy rooma\n",
      "kamar mandi kurang bersih kurang lengkap faslitasnya kamar mandi yang lumayan\n",
      "lokasi susah temu dari luar lihat nama proses reservasi sedikit lama roomnya nyaman bersih\n",
      "sarung bantal bau\n",
      "kamar luas bersih makan kurang\n",
      "lumayan lah sangat jangkau perlu yg benah soal bersih kasur kemarin dapat kasur jorok banyak helai rambur seperti bersih ganti seprei bekas unjung belum\n",
      "suasana kmr yg prlu direfresh dn agk sempit ukrn kmr\n",
      "layan cukup baik tempat kurang strategis\n",
      "kesan pertama dapat cukup bingung karena hotel nama airy nyata ada dalam bhotel tolong saran tambah informasi sehingga orang yang mes kali langsung eta\n",
      "kamar mandi bau lubang salur air mampat penuh rambut harus yang bersih kurang meja kecil samping tempat tidur kamar kecil sangat nyaman\n",
      "segi fasilitas nya sekarang kurang kamar mandi nya showernya ada ikat layan sedikit lebih d tingkat lagi\n",
      "kamar nyaman acnya berbunyilantai kamar kurang bersih makan unuk sarap enak terima kasih airy\n",
      "lumayan bersih ramah cuma kamar kedap suara betul kamar dekat dng resepsionis klo resepsionis bicara dengar kamar\n",
      "pas hujan air masuk kamar hotel\n",
      "bau rokok\n",
      "sangat emejing inap airy sutoyo hotel dewarnaudah murah tempat nyaman buat istirahatapalagi breakfastnya nikmat ga asal harga murah kualitas murahantp yg perlu baik cuman bersih kamar kurang bersih klo ngepeloverall rekomen bgt\n",
      "kamar spt kos kos an ada handuk ada hot water parkir mobil sempit\n",
      "sprei kasur handuk kurangp bersih tetapiklo lebih bersih tambah oke\n",
      "ada layan tonton televisi lain muas oke\n",
      "cukup oke pelayanannyacuma handuk kurang bersih terima kasih\n",
      "sangat muas sprei handuk wangi kamar cukup bersih nyaman ac air panas fungsi baik salur tv sedikit masalah cukup muas alam dua di airy gunung sahari terimakasih\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/jordhy97/final_project/master/data/reviews/reviews.txt\"\n",
    "file = urllib.request.urlopen(url)\n",
    "\n",
    "stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "aggregated_lines = \"\"\n",
    "\n",
    "for line in file:\n",
    "    decoded_line = line.decode(\"utf-8\")\n",
    "    lower_decoded_line = decoded_line.lower()\n",
    "    num_rem_decoded_line = re.sub(r\"\\d+\", \"\", lower_decoded_line)\n",
    "    punc_rem_decoded_line = num_rem_decoded_line.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    sw_decoded_line = stopword.remove(punc_rem_decoded_line)\n",
    "    katadasar_line = stemmer.stem(sw_decoded_line)\n",
    "    print(katadasar_line)\n",
    "    aggregated_lines = aggregated_lines + \"[\" +aggregated_lines + \"], \" +\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first build a graph associated with the text, where the graph vertices are representative for the units to be ranked. The goal is to rank entire sentences, therefore, a vertex is added to the graph for each sentence in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some conditions a good similarity measure has to obey:\n",
    "\n",
    "1. 0 <= similarity(A, B) <= 1\n",
    "2. similarity(A, A) = 1\n",
    "3. similarity(A, B) =/= 1 if A =/= B\n",
    "\n",
    "Note: A and B are different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used measure in Natural Language Processing is the Cosine Similarity. The Cosine Similarity computes the cosine of the angle between 2 vectors. If the vectors are identical, the cosine is 1.0. If the vectors are orthogonal, the cosine is 0.0. This means the cosine similarity is a measure we can use. NLTK implements cosine_distance, which is 1 - cosine_similarity. The concept of distance is opposite to similarity. Two identical vectors are located at 0 distance and are 100% similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Similarity using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(aggregated_lines, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(aggregated_lines), len(aggregated_lines)))\n",
    " \n",
    "    for idx1 in range(len(aggregated_lines)):\n",
    "        for idx2 in range(len(aggregated_lines)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(aggregated_file[idx1], aggregated_file[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " \n",
    "print(\"Len of aggregated lines: \" + len(aggregated_lines))\n",
    "\n",
    "stop_words = \"\"\n",
    "S = build_similarity_matrix(aggregated_lines, stop_words)    \n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ranks = pagerank(S)\n",
    " \n",
    "print(sentence_ranks)\n",
    " \n",
    "# Get the sentences ordered by rank\n",
    "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "print(ranked_sentence_indexes)\n",
    " \n",
    "# Suppose we want the 5 most import sentences\n",
    "SUMMARY_SIZE = 5\n",
    "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
    "print(SELECTED_SENTENCES)\n",
    " \n",
    "# Fetch the most important sentences\n",
    "summary = itemgetter(*SELECTED_SENTENCES)(aggregated_lines)\n",
    " \n",
    "# Print the actual summary\n",
    "for aggregated_line in summary:\n",
    "    print(' '.join(aggregated_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(aggregated_lines, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    aggregated_lines = a list of airy review sentences \n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(aggregated_lines, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(aggregated_lines)\n",
    "    return summary\n",
    " \n",
    "for idx, aggregated_line in enumerate(textrank(aggregated_lines, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(aggregated_line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the purpose of this project, we will be using ROUGE and BLEU metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Reference\n",
    "\n",
    "- Preprocessing techniques ideas are inspired by the links below: \n",
    "    1. https://www.academia.edu/35015140/Preprocessing_Techniques_for_Text_Mining\n",
    "    2. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "\n",
    "- References for library for Indonesian Stopwords: \n",
    "    1. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "    2. https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\n",
    "    \n",
    "- Code for the modelling of the project is mainly referred from this website: \n",
    "    1. https://nlpforhackers.io/textrank-text-summarization/ \n",
    "    2. I do not take ownership of the code presented above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reference\n",
    "\n",
    "Below are links I have referred to throughout the making of the project:\n",
    "- TextRank algorithms: \n",
    "    1. https://iq.opengenus.org/textrank-for-text-summarization/\n",
    "    2. https://github.com/summanlp/textrank\n",
    "    3. https://github.com/davidadamojr/TextRank\n",
    "    4. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "    5. https://medium.com/@shivangisareen/text-summarisation-with-gensim-textrank-46bbb3401289\n",
    "    6. https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "- Text Summarization algorithm using word frequency: \n",
    "    1. https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
