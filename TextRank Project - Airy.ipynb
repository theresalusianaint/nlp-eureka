{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "What:\n",
    "TextRank uses an extractive approach and is an unsupervised graph-based text summarization technique. \n",
    "\n",
    "How: \n",
    "The basic idea implemented by a graph-based ranking model is that of voting or recommendation.\n",
    "When one vertex links to another one, it is basically casting a vote for that vertex. The higher the number of votes cast for a vertex, the higher the importance of that vertex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "untuk kelancaran TextRank ini, diperlukan untuk :\n",
    "1. men-download library stopwords Bahasa Indonesia: Python Sastrawi.\n",
    "    Cara download library:\n",
    "    1. Open terminal\n",
    "    2. write on command line : \"pip install Sastrawi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "import urllib\n",
    "import os, os.path\n",
    "#import modular regular expression\n",
    "import re\n",
    "import string\n",
    "# import StopWordRemoverFactory class\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open txt file and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing methods used:\n",
    "1. decode to UTF-8\n",
    "2. change all to lower case -> consistency purposes especially during stopwords removal process\n",
    "3. remove all numbers -> avoid redundancy\n",
    "4. remove all punctuation -> avoid redundancy\n",
    "5. stopwords (using Python Sastrawi) -> avoid redundancy\n",
    "6. Stemmer (using Python Sastrawi) -> easy filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jordhy97/final_project/master/data/reviews/reviews.txt\"\n",
    "file = urllib.request.urlopen(url)\n",
    "\n",
    "stopword = StopWordRemoverFactory().create_stop_word_remover()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "aggregated_file = \"\"\n",
    "\n",
    "for line in file:\n",
    "    decoded_line = line.decode(\"utf-8\")\n",
    "    lower_decoded_line = decoded_line.lower()\n",
    "    num_rem_decoded_line = re.sub(r\"\\d+\", \"\", lower_decoded_line)\n",
    "    punc_rem_decoded_line = num_rem_decoded_line.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    sw_decoded_line = stopword.remove(punc_rem_decoded_line)\n",
    "    katadasar_line = stemmer.stem(sw_decoded_line)\n",
    "    print(katadasar_line)\n",
    "    aggregated_lines = aggregated_lines + \"[\" +aggregated_lines + \"], \" +\"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first build a graph associated with the text, where the graph vertices are representative for the units to be ranked. The goal is to rank entire sentences, therefore, a vertex is added to the graph for each sentence in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some conditions a good similarity measure has to obey:\n",
    "\n",
    "1. 0 <= similarity(A, B) <= 1\n",
    "2. similarity(A, A) = 1\n",
    "3. similarity(A, B) =/= 1 if A =/= B\n",
    "\n",
    "Note: A and B are different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used measure in Natural Language Processing is the Cosine Similarity. The Cosine Similarity computes the cosine of the angle between 2 vectors. If the vectors are identical, the cosine is 1.0. If the vectors are orthogonal, the cosine is 0.0. This means the cosine similarity is a measure we can use. NLTK implements cosine_distance, which is 1 - cosine_similarity. The concept of distance is opposite to similarity. Two identical vectors are located at 0 distance and are 100% similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Similarity using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(aggregated_lines, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(aggregated_lines), len(aggregated_lines)))\n",
    " \n",
    "    for idx1 in range(len(aggregated_lines)):\n",
    "        for idx2 in range(len(aggregated_lines)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(aggregated_file[idx1], aggregated_file[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " \n",
    "print(\"Len of aggregated lines: \" + len(aggregated_lines))\n",
    "\n",
    "stop_words = \"\"\n",
    "S = build_similarity_matrix(aggregated_lines, stop_words)    \n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ranks = pagerank(S)\n",
    " \n",
    "print(sentence_ranks)\n",
    " \n",
    "# Get the sentences ordered by rank\n",
    "ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "print(ranked_sentence_indexes)\n",
    " \n",
    "# Suppose we want the 5 most import sentences\n",
    "SUMMARY_SIZE = 5\n",
    "SELECTED_SENTENCES = sorted(ranked_sentence_indexes[:SUMMARY_SIZE])\n",
    "print(SELECTED_SENTENCES)\n",
    " \n",
    "# Fetch the most important sentences\n",
    "summary = itemgetter(*SELECTED_SENTENCES)(aggregated_lines)\n",
    " \n",
    "# Print the actual summary\n",
    "for aggregated_line in summary:\n",
    "    print(' '.join(aggregated_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(aggregated_lines, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    aggregated_lines = a list of airy review sentences \n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(aggregated_lines, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(aggregated_lines)\n",
    "    return summary\n",
    " \n",
    "for idx, aggregated_line in enumerate(textrank(aggregated_lines, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(aggregated_line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the purpose of this project, we will be using ROUGE and BLEU metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Preprocessing techniques: \n",
    "    1. https://www.academia.edu/35015140/Preprocessing_Techniques_for_Text_Mining\n",
    "    2. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "- Library for Indonesian Stopwords: \n",
    "    1. https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n",
    "    2. https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\n",
    "- TextRank algorithms: \n",
    "    1. https://iq.opengenus.org/textrank-for-text-summarization/\n",
    "    2. https://github.com/summanlp/textrank\n",
    "    3. https://github.com/davidadamojr/TextRank\n",
    "    4. https://nlpforhackers.io/textrank-text-summarization/\n",
    "    5. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "    6. https://medium.com/@shivangisareen/text-summarisation-with-gensim-textrank-46bbb3401289\n",
    "- Text Summarization algorithm using word frequency: \n",
    "    1. https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
